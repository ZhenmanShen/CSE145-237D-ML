# -*- coding: utf-8 -*-
"""train_proxylessNAS_Abhay.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eHrHyeArcaNzpKv5rbyq43rfJBdZSju6
"""

!pip install -U gdown
import gdown

url = 'https://drive.google.com/file/d/1z4LC1nY65FcO9H7Y--weWk6tycSJk8R-/view?usp=sharing'
file_id = url.split('/d/')[1].split('/')[0]
download_url = f'https://drive.google.com/uc?id={file_id}'

output_file = 'data.zip'
gdown.download(download_url, output_file, quiet=False)

!unzip data.zip

import requests

# This is the direct download link constructed from your SharePoint link
url = 'https://ucsdcloud-my.sharepoint.com/personal/ablal_ucsd_edu/_layouts/15/download.aspx?share=EavH0dkbioFKj6EWSgzL61IBYTYRdDFFowXfcQWsF-wJdQ'

output_file = 'data.zip'

response = requests.get(url, stream=True)
if response.status_code == 200:
    with open(output_file, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    print(f"Downloaded successfully to '{output_file}'")
else:
    print(f"Failed to download. Status code: {response.status_code}")

import os
import torch
import torchaudio
import pandas as pd
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchaudio.transforms import MelSpectrogram, AmplitudeToDB
from tqdm import tqdm

https://ucsdcloud-my.sharepoint.com/:u:/g/personal/ablal_ucsd_edu/EavH0dkbioFKj6EWSgzL61IBYTYRdDFFowXfcQWsF-wJdQ?e=EXoG5O

# Configuration
torchaudio.set_audio_backend("soundfile")

AUDIO_DIR = "/content/buowset/audio"
METADATA_CSV = "/content/buowset/meta/metadata.csv"
OUTPUT_DIR = "precomputed_mels"
SAMPLE_RATE = 44100
TARGET_DURATION = 3  # seconds
NUM_MELS = 64
HOP_LENGTH = 512
N_FFT = 1024

# Mel spectrogram transformation
mel_transform = torch.nn.Sequential(
    MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=NUM_MELS),
    AmplitudeToDB()
)

def process_audio(file_path):
    waveform, sr = torchaudio.load(file_path)

    # Convert stereo to mono if needed
    if waveform.shape[0] > 1:
        waveform = waveform.mean(dim=0, keepdim=True)

    if sr != SAMPLE_RATE:
        waveform = torchaudio.functional.resample(waveform, sr, SAMPLE_RATE)

    # Crop/pad to fixed duration
    max_len = SAMPLE_RATE * TARGET_DURATION
    if waveform.shape[1] < max_len:
        waveform = torch.nn.functional.pad(waveform, (0, max_len - waveform.shape[1]))
    else:
        waveform = waveform[:, :max_len]

    mel = mel_transform(waveform)  # shape: (1, 64, T)
    mel_mean = mel.mean()
    mel_std = mel.std()
    mel = (mel - mel_mean) / (mel_std + 1e-6)  # z-score normalization
    return mel

def main():
    df = pd.read_csv(METADATA_CSV)
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    for fold in sorted(df["fold"].unique()):
        os.makedirs(os.path.join(OUTPUT_DIR, f"fold{fold}"), exist_ok=True)

    for _, row in tqdm(df.iterrows(), total=len(df)):
        audio_file = os.path.join(AUDIO_DIR, row["segment"])
        fold = row["fold"]
        label = int(row["label"])
        out_path = os.path.join(OUTPUT_DIR, f"fold{fold}", row["segment"].replace(".wav", ".pt"))

        try:
            mel = process_audio(audio_file)
            torch.save({"mel": mel, "label": label}, out_path)
        except Exception as e:
            print(f"Failed {audio_file}: {e}")

if __name__ == "__main__":
    main()

!git clone https://github.com/mit-han-lab/proxylessnas.git
!cd proxylessnas

!pwd

import sys
sys.path.append('/content/proxylessnas/proxyless_nas')  # adjust path

pwd

from proxylessnas.proxyless_nas.model_zoo import proxyless_mobile
net = proxyless_mobile()  # returns a ready-to-train nn.Module

model = proxyless_mobile()

ROOT_DIR = "precomputed_mels"
VAL_FOLD = 0
BATCH_SIZE = 16
EPOCHS = 10
NUM_CLASSES = 6
LEARNING_RATE = 0.001
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class BUOWMelDataset(Dataset):
    def __init__(self, root_dir, val_fold=0, train=True):
        self.samples = []
        fold_prefix = f"fold{val_fold}"

        for fold_name in os.listdir(root_dir):
            if (train and fold_name != fold_prefix) or (not train and fold_name == fold_prefix):
                fold_path = os.path.join(root_dir, fold_name)
                for fname in os.listdir(fold_path):
                    if fname.endswith(".pt"):
                        self.samples.append(os.path.join(fold_path, fname))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = torch.load(self.samples[idx])
        mel = sample["mel"]          # shape: (1, 64, 258)
        label = sample["label"]
        return mel, label

class ProxylessAudioClassifier(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.model = proxyless_mobile()

        # Modify input layer for 1-channel input (Mel spectrogram)
        self.model.first_conv = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True)
        )

        # Modify output classifier
        self.model.classifier = nn.Linear(self.model.classifier.in_features, num_classes)

    def forward(self, x):
        return self.model(x)

from sklearn.metrics import precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import pandas as pd

def train_model():
    train_dataset = BUOWMelDataset(ROOT_DIR, val_fold=VAL_FOLD, train=True)
    val_dataset = BUOWMelDataset(ROOT_DIR, val_fold=VAL_FOLD, train=False)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

    model = ProxylessAudioClassifier(NUM_CLASSES).to(DEVICE)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # Metrics storage
    train_acc_list = []
    val_acc_list = []
    train_loss_list = []
    precision_list = []
    recall_list = []
    f1_list = []

    for epoch in range(EPOCHS):
        model.train()
        running_loss = 0.0
        correct, total = 0, 0

        for inputs, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}"):
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

        train_loss = running_loss
        train_acc = correct / total
        train_loss_list.append(train_loss)
        train_acc_list.append(train_acc)

        print(f"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}")

        # ─── Validation ───
        model.eval()
        correct, total = 0, 0
        all_preds, all_labels = [], []

        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
                outputs = model(inputs)
                preds = outputs.argmax(dim=1)

                correct += (preds == labels).sum().item()
                total += labels.size(0)

                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

        val_acc = correct / total
        val_acc_list.append(val_acc)

        # Precision, Recall, F1 (macro avg for multi-class)
        precision = precision_score(all_labels, all_preds, average="macro", zero_division=0)
        recall = recall_score(all_labels, all_preds, average="macro", zero_division=0)
        f1 = f1_score(all_labels, all_preds, average="macro", zero_division=0)

        precision_list.append(precision)
        recall_list.append(recall)
        f1_list.append(f1)

        print(f"Val Acc: {val_acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}")

    # ─── Save Model ───
    torch.save(model.state_dict(), "buow_proxylessnas.pth")
    print("Saved model to buow_proxylessnas.pth")

    # ─── Save Metrics ───
    metrics_df = pd.DataFrame({
        "epoch": list(range(1, EPOCHS + 1)),
        "train_loss": train_loss_list,
        "train_accuracy": train_acc_list,
        "val_accuracy": val_acc_list,
        "precision": precision_list,
        "recall": recall_list,
        "f1_score": f1_list
    })
    metrics_df.to_csv("training_metrics.csv", index=False)
    print(" Saved training metrics to training_metrics.csv")

    # ─── Plot & Save ───
    plt.figure()
    plt.plot(metrics_df["epoch"], metrics_df["train_loss"], label="Train Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Training Loss")
    plt.legend()
    plt.savefig("train_loss.png")
    plt.close()

    plt.figure()
    plt.plot(metrics_df["epoch"], metrics_df["train_accuracy"], label="Train Accuracy")
    plt.plot(metrics_df["epoch"], metrics_df["val_accuracy"], label="Val Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.title("Train vs Validation Accuracy")
    plt.legend()
    plt.savefig("accuracy_curve.png")
    plt.close()

    plt.figure()
    plt.plot(metrics_df["epoch"], metrics_df["precision"], label="Precision")
    plt.plot(metrics_df["epoch"], metrics_df["recall"], label="Recall")
    plt.plot(metrics_df["epoch"], metrics_df["f1_score"], label="F1 Score")
    plt.xlabel("Epoch")
    plt.ylabel("Score")
    plt.title("Precision, Recall, F1 over Epochs")
    plt.legend()
    plt.savefig("precision_recall_f1.png")
    plt.close()

    print("Saved plots: train_loss.png, accuracy_curve.png, precision_recall_f1.png")


if __name__ == "__main__":
    train_model()

!zip -r precomputed_mels.zip precomputed_mels



import torch
import torch.nn as nn
from proxyless_nas.model_zoo import proxyless_mobile

# Wrapper class (same as during training)
class ProxylessAudioClassifier(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.model = proxyless_mobile()
        self.model.first_conv = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True)
        )
        self.model.classifier = nn.Linear(self.model.classifier.in_features, num_classes)

    def forward(self, x):
        return self.model(x)

# Instantiate and load
model = ProxylessAudioClassifier(num_classes=6)
model.load_state_dict(torch.load("buow_proxylessnas.pth"))
model.eval()

pip install onnx

import torch

# Dummy input should match the model input shape: [B, C, H, W]
dummy_input = torch.randn(1, 1, 64, 258)  # [batch_size, channels, mel_bins, time_frames]

torch.onnx.export(
    model,
    dummy_input,
    "proxyless_buow.onnx",             # Output file name
    input_names=["input"],              # Input name for ONNX graph
    output_names=["output"],            # Output name for ONNX graph
    opset_version=11,                   # Good default for STM32Cube.AI compatibility
    dynamic_axes={"input": {0: "batch"}, "output": {0: "batch"}}
)

print(" Exported to proxyless_buow.onnx")

pip install onnx onnx-tf tensorflow

import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_saved_model("proxyless_buow_tf")
converter.optimizations = [tf.lite.Optimize.DEFAULT]  # For quantization

# Optional: int8 quantization
# converter.representative_dataset = your_calibration_fn
# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]

tflite_model = converter.convert()

with open("proxyless_buow.tflite", "wb") as f:
    f.write(tflite_model)

xxd -i proxyless_buow.tflite > buow_model_data.h

